{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/richard/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/richard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "PERCENTAGE_DOCS = 1.0 # random subsample of Reuters training docs\n",
    "VOCAB_SIZE = 1000\n",
    "REMOVE_TOP_K_TERMS = 50\n",
    "MIN_TERM_FREQ = 3\n",
    "TEXT_WINDOW_SIZE = 8\n",
    "BATCH_SIZE = 10 * TEXT_WINDOW_SIZE\n",
    "EMBEDDING_SIZE = 128\n",
    "PV_TEST_SET_PERCENTAGE = 5\n",
    "NUM_STEPS = 10001\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_SAMPLED = 64\n",
    "REPORT_EVERY_X_STEPS = 200\n",
    "SHUFFLE_EVERY_X_EPOCH = 5\n",
    "\n",
    "# Token integer ids for special tokens\n",
    "UNK = 0\n",
    "NULL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns an eternal generator, periodically shuffling the order\n",
    "\n",
    "l_ is a list of integers; an internal copy of it is maintained.\n",
    "\"\"\"\n",
    "def repeater_shuffler(l_):\n",
    "    l = np.array(l_, dtype=np.int32)\n",
    "    epoch = 0\n",
    "    while epoch >= 0:\n",
    "        if epoch % SHUFFLE_EVERY_X_EPOCH == 0:\n",
    "            np.random.shuffle(l)\n",
    "        for i in l:\n",
    "            yield i\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accept_doc(fileid):\n",
    "    return fileid.startswith('training/') \\\n",
    "            and np.random.random() * 100 < PERCENTAGE_DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accept(word):\n",
    "    # Accept if not only Unicode non-word characters are present\n",
    "    return re.sub(r'\\W', '', word) != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(word):\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    fileid2words = {fileid:\n",
    "            [normalize(word) for word in word_tokenize(\n",
    "                    reuters.raw(fileid)) if accept(word)] \\\n",
    "            for fileid in reuters.fileids() if accept_doc(fileid)}\n",
    "    count = [['__UNK__', 0], ['__NULL__', 0]]\n",
    "    count.extend([(word, count) for word, count in collections.Counter(\n",
    "            [word for words in fileid2words.values() \\\n",
    "            for word in words]).most_common(\n",
    "                    VOCAB_SIZE - 2 + REMOVE_TOP_K_TERMS)[\n",
    "                            REMOVE_TOP_K_TERMS:\n",
    "                    ] if count >= MIN_TERM_FREQ])\n",
    "    assert not set(['__UNK__', '__NULL__']) & set(next(zip(\n",
    "            *count[2:])))\n",
    "    dictionary = {}\n",
    "    for i, (word, _) in enumerate(count):\n",
    "        dictionary[word] = i\n",
    "    reverse_dictionary = dict(zip(dictionary.values(),\n",
    "                                  dictionary.keys()))\n",
    "    data = []\n",
    "    doclens = []\n",
    "    fileids = []\n",
    "    for docid, (fileid, words) in enumerate(fileid2words.items()):\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                wordid = dictionary[word]\n",
    "            else:\n",
    "                wordid = UNK\n",
    "                count[UNK][1] += 1\n",
    "            data.append((docid, wordid))\n",
    "        # Pad with NULL values if necessary\n",
    "        doclen = len(words)\n",
    "        doclens.append(doclen)\n",
    "        fileids.append(fileid)\n",
    "        if doclen < TEXT_WINDOW_SIZE:\n",
    "            n_nulls = TEXT_WINDOW_SIZE - doclen\n",
    "            data.extend([(docid, NULL)] * n_nulls)\n",
    "            count[NULL][1] += n_nulls\n",
    "    return data, count, doclens, fileids, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, count, doclens, fileids, dictionary, reverse_dictionary = \\\n",
    "        build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 71\n",
      "Number of tokens: 9954\n",
      "Number of unique tokens: 612\n",
      "Most common words (+UNK and NULL): [['__UNK__', 6196], ['__NULL__', 6], ('one', 25), ('japan', 23), ('were', 23)]\n",
      "Least common words: [('tagamet', 3), ('willing', 3), ('signs', 3), ('show', 3), ('gmt', 3)]\n",
      "Sample data: [(0, 0), (0, 0), (0, 1), (0, 1), (0, 1)]\n",
      "Effective vocab size: 612\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents:', len(set(next(zip(*data)))))\n",
    "print('Number of tokens:', len(data))\n",
    "print('Number of unique tokens:', len(count))\n",
    "assert len(data) == sum([i for _, i in count])\n",
    "print('Most common words (+UNK and NULL):', count[:5])\n",
    "print('Least common words:', count[-5:])\n",
    "print('Sample data:', data[:5])\n",
    "vocab_size = min(VOCAB_SIZE, len(count))\n",
    "print('Effective vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     71.000000\n",
       "mean     140.112676\n",
       "std      145.936537\n",
       "min        2.000000\n",
       "25%       53.000000\n",
       "50%       99.000000\n",
       "75%      177.000000\n",
       "max      858.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(doclens).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_window_center_positions():\n",
    "    # If TEXT_WINDOW_SIZE is even, then define text_window_center\n",
    "    # as left-of-middle-pair\n",
    "    doc_start_indexes = [0]\n",
    "    last_docid = data[0][0]\n",
    "    for i, (d, _) in enumerate(data):\n",
    "        if d != last_docid:\n",
    "            doc_start_indexes.append(i)\n",
    "            last_docid = d\n",
    "    twcp = []\n",
    "    for i in range(len(doc_start_indexes) - 1):\n",
    "        twcp.extend(list(range(\n",
    "                doc_start_indexes[i] + (TEXT_WINDOW_SIZE - 1) // 2,\n",
    "                doc_start_indexes[i + 1] - TEXT_WINDOW_SIZE // 2\n",
    "                )))\n",
    "    return doc_start_indexes, twcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_start_indexes, twcp = get_text_window_center_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test():\n",
    "    split_point = (len(twcp) // 100) * (100 - PV_TEST_SET_PERCENTAGE)\n",
    "    twcp_train = twcp[:split_point]\n",
    "\n",
    "    # Test set data must come from known documents\n",
    "    docids_train = set([data[i][0] for i in twcp_train])\n",
    "    twcp_test = []\n",
    "    for i in twcp[split_point:]:\n",
    "        if data[i][0] in docids_train:\n",
    "            twcp_test.append(i)\n",
    "        else:\n",
    "            twcp_train.append(i)\n",
    "    if len(twcp_test) < (BATCH_SIZE // TEXT_WINDOW_SIZE):\n",
    "        raise ValueError(\n",
    "            'Too little test data, try increasing PV_TEST_SET_PERCENTAGE')\n",
    "    return twcp_train, twcp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(twcp)\n",
    "twcp_train, twcp_test = get_train_test()\n",
    "twcp_train_gen = repeater_shuffler(twcp_train)\n",
    "del twcp # save some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective test set percentage: 473 out of 9403, 5.0%\n"
     ]
    }
   ],
   "source": [
    "print('Effective test set percentage: {} out of {}, {:.1f}%'.format(\n",
    "        len(twcp_test), len(twcp_test) + len(twcp_train),\n",
    "        100 * len(twcp_test) / (len(twcp_test) + len(twcp_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del twcp_train # save some memory, we use twcp_train_gen from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch_single_twcp(twcp, i, batch, labels):\n",
    "    tw_start = twcp - (TEXT_WINDOW_SIZE - 1) // 2\n",
    "    tw_end = twcp + TEXT_WINDOW_SIZE // 2 + 1\n",
    "    docids, wordids = zip(*data[tw_start:tw_end])\n",
    "    batch_slice = slice(i * TEXT_WINDOW_SIZE,\n",
    "                        (i+1) * TEXT_WINDOW_SIZE)\n",
    "    batch[batch_slice] = docids\n",
    "    labels[batch_slice, 0] = wordids\n",
    "    \n",
    "def generate_batch(twcp_gen):\n",
    "    batch = np.ndarray(shape=(BATCH_SIZE,), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(BATCH_SIZE, 1), dtype=np.int32)\n",
    "    for i in range(BATCH_SIZE // TEXT_WINDOW_SIZE):\n",
    "        generate_batch_single_twcp(next(twcp_gen), i, batch, labels)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "dataset = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "\n",
    "# Weights\n",
    "embeddings = tf.Variable(\n",
    "        tf.random_uniform([len(doclens), EMBEDDING_SIZE],\n",
    "                          -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "                [vocab_size, EMBEDDING_SIZE],\n",
    "                stddev=1.0 / np.sqrt(EMBEDDING_SIZE)))\n",
    "softmax_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "# Model\n",
    "# Look up embeddings for inputs\n",
    "embed = tf.nn.embedding_lookup(embeddings, dataset)\n",
    "# Compute the softmax loss, using a sample of the negative\n",
    "# labels each time\n",
    "loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(\n",
    "                softmax_weights, softmax_biases, embed,\n",
    "                labels, NUM_SAMPLED, vocab_size))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(\n",
    "        loss)\n",
    "\n",
    "# Test loss\n",
    "test_loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                tf.matmul(embed, tf.transpose(\n",
    "                          softmax_weights)) + softmax_biases,\n",
    "                labels[:, 0]))\n",
    "\n",
    "# Normalized embeddings (to use cosine similarity later on)\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1,\n",
    "                              keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_loss(session):\n",
    "    # We do this in batches, too, to keep memory usage low.\n",
    "    # Since our graph works with a fixed batch size, we\n",
    "    # are lazy and just compute test loss on all batches that\n",
    "    # fit in the test set.\n",
    "    twcp_test_gen = (i for i in twcp_test)\n",
    "    n_batches = (len(twcp_test) * TEXT_WINDOW_SIZE) // BATCH_SIZE\n",
    "    test_l = 0.0\n",
    "    for _ in range(n_batches):\n",
    "        batch_data, batch_labels = generate_batch(twcp_test_gen)\n",
    "        test_l += session.run([test_loss], feed_dict={\n",
    "                dataset: batch_data, labels: batch_labels\n",
    "            })[0]\n",
    "    return test_l / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(session):\n",
    "    avg_training_loss = 0\n",
    "    for step in range(NUM_STEPS):\n",
    "        batch_data, batch_labels = generate_batch(twcp_train_gen)\n",
    "        _, l = session.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={dataset: batch_data, labels: batch_labels})\n",
    "        avg_training_loss += l\n",
    "        if step > 0 and step % REPORT_EVERY_X_STEPS == 0:\n",
    "            avg_training_loss = \\\n",
    "                    avg_training_loss / REPORT_EVERY_X_STEPS\n",
    "            # The average loss is an estimate of the loss over the\n",
    "            # last REPORT_EVERY_X_STEPS batches\n",
    "            print('Average loss at step {:d}: {:.1f}'.format(\n",
    "                    step, avg_training_loss))\n",
    "            avg_training_loss = 0\n",
    "            test_l = get_test_loss(session)\n",
    "            print('Test loss at step {:d}: {:.1f}'.format(\n",
    "                    step, test_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 200: 2.5\n",
      "Test loss at step 200: 2.9\n",
      "Average loss at step 400: 2.0\n",
      "Test loss at step 400: 2.7\n",
      "Average loss at step 600: 1.9\n",
      "Test loss at step 600: 2.6\n",
      "Average loss at step 800: 1.8\n",
      "Test loss at step 800: 2.5\n",
      "Average loss at step 1000: 1.7\n",
      "Test loss at step 1000: 2.5\n",
      "Average loss at step 1200: 1.6\n",
      "Test loss at step 1200: 2.4\n",
      "Average loss at step 1400: 1.6\n",
      "Test loss at step 1400: 2.4\n",
      "Average loss at step 1600: 1.5\n",
      "Test loss at step 1600: 2.3\n",
      "Average loss at step 1800: 1.5\n",
      "Test loss at step 1800: 2.3\n",
      "Average loss at step 2000: 1.5\n",
      "Test loss at step 2000: 2.3\n",
      "Average loss at step 2200: 1.5\n",
      "Test loss at step 2200: 2.3\n",
      "Average loss at step 2400: 1.5\n",
      "Test loss at step 2400: 2.3\n",
      "Average loss at step 2600: 1.4\n",
      "Test loss at step 2600: 2.3\n",
      "Average loss at step 2800: 1.4\n",
      "Test loss at step 2800: 2.2\n",
      "Average loss at step 3000: 1.4\n",
      "Test loss at step 3000: 2.2\n",
      "Average loss at step 3200: 1.4\n",
      "Test loss at step 3200: 2.2\n",
      "Average loss at step 3400: 1.4\n",
      "Test loss at step 3400: 2.2\n",
      "Average loss at step 3600: 1.4\n",
      "Test loss at step 3600: 2.2\n",
      "Average loss at step 3800: 1.4\n",
      "Test loss at step 3800: 2.2\n",
      "Average loss at step 4000: 1.4\n",
      "Test loss at step 4000: 2.2\n",
      "Average loss at step 4200: 1.4\n",
      "Test loss at step 4200: 2.2\n",
      "Average loss at step 4400: 1.4\n",
      "Test loss at step 4400: 2.2\n",
      "Average loss at step 4600: 1.4\n",
      "Test loss at step 4600: 2.2\n",
      "Average loss at step 4800: 1.4\n",
      "Test loss at step 4800: 2.2\n",
      "Average loss at step 5000: 1.3\n",
      "Test loss at step 5000: 2.2\n",
      "Average loss at step 5200: 1.3\n",
      "Test loss at step 5200: 2.1\n",
      "Average loss at step 5400: 1.4\n",
      "Test loss at step 5400: 2.1\n",
      "Average loss at step 5600: 1.4\n",
      "Test loss at step 5600: 2.1\n",
      "Average loss at step 5800: 1.3\n",
      "Test loss at step 5800: 2.1\n",
      "Average loss at step 6000: 1.3\n",
      "Test loss at step 6000: 2.1\n",
      "Average loss at step 6200: 1.3\n",
      "Test loss at step 6200: 2.1\n",
      "Average loss at step 6400: 1.3\n",
      "Test loss at step 6400: 2.1\n",
      "Average loss at step 6600: 1.3\n",
      "Test loss at step 6600: 2.1\n",
      "Average loss at step 6800: 1.3\n",
      "Test loss at step 6800: 2.1\n",
      "Average loss at step 7000: 1.3\n",
      "Test loss at step 7000: 2.1\n",
      "Average loss at step 7200: 1.3\n",
      "Test loss at step 7200: 2.1\n",
      "Average loss at step 7400: 1.3\n",
      "Test loss at step 7400: 2.1\n",
      "Average loss at step 7600: 1.3\n",
      "Test loss at step 7600: 2.1\n",
      "Average loss at step 7800: 1.3\n",
      "Test loss at step 7800: 2.1\n",
      "Average loss at step 8000: 1.3\n",
      "Test loss at step 8000: 2.1\n",
      "Average loss at step 8200: 1.3\n",
      "Test loss at step 8200: 2.1\n",
      "Average loss at step 8400: 1.3\n",
      "Test loss at step 8400: 2.1\n",
      "Average loss at step 8600: 1.3\n",
      "Test loss at step 8600: 2.1\n",
      "Average loss at step 8800: 1.3\n",
      "Test loss at step 8800: 2.1\n",
      "Average loss at step 9000: 1.3\n",
      "Test loss at step 9000: 2.1\n",
      "Average loss at step 9200: 1.3\n",
      "Test loss at step 9200: 2.1\n",
      "Average loss at step 9400: 1.3\n",
      "Test loss at step 9400: 2.1\n",
      "Average loss at step 9600: 1.3\n",
      "Test loss at step 9600: 2.1\n",
      "Average loss at step 9800: 1.3\n",
      "Test loss at step 9800: 2.1\n",
      "Average loss at step 10000: 1.3\n",
      "Test loss at step 10000: 2.1\n"
     ]
    }
   ],
   "source": [
    "%lprun -f train train(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common class in sampled documents: earn\n"
     ]
    }
   ],
   "source": [
    "most_common_class = collections.Counter(\n",
    "        [c for cs in [reuters.categories(fileid) for fileid in fileids] \\\n",
    "        for c in cs]).most_common(1)[0][0]\n",
    "print('Most common class in sampled documents:', most_common_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tc_labels = [1 if most_common_class in reuters.categories(fileid) else 0 \\\n",
    "             for fileid in fileids]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
