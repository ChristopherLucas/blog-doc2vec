{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/richard/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/richard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "N_DOCS = 100 # Only use first N_DOCS Reuters docs\n",
    "VOCAB_SIZE = 50000\n",
    "TEXT_WINDOW_SIZE = 8\n",
    "BATCH_SIZE = 10 * TEXT_WINDOW_SIZE\n",
    "EMBEDDING_SIZE = 128\n",
    "PV_TEST_SET_PERCENTAGE = 5\n",
    "NUM_STEPS = 100001\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_SAMPLED = 64\n",
    "REPORT_EVERY_X_STEPS = 100\n",
    "\n",
    "# Token integer ids for special tokens\n",
    "UNK = 0\n",
    "NULL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accept(word):\n",
    "    # Accept if not only Unicode non-word characters are present\n",
    "    return re.sub(r'\\W', '', word) != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(word):\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    doc2words = {docid: [normalize(word) for word in word_tokenize(\n",
    "            reuters.raw(fileid)) if accept(word)] \\\n",
    "            for docid, fileid in enumerate(\n",
    "                    reuters.fileids()[:N_DOCS])}\n",
    "    count = [['__UNK__', 0], ['__NULL__', 0]]\n",
    "    count.extend(collections.Counter(\n",
    "            [word for words in doc2words.values() \\\n",
    "                    for word in words]).most_common(VOCAB_SIZE - 2))\n",
    "    assert not set(['__UNK__', '__NULL__']) & set(next(zip(\n",
    "            *count[2:])))\n",
    "    dictionary = {}\n",
    "    for i, (word, _) in enumerate(count):\n",
    "        dictionary[word] = i\n",
    "    reverse_dictionary = dict(zip(dictionary.values(),\n",
    "                                  dictionary.keys()))\n",
    "    data = []\n",
    "    doclens = []\n",
    "    for docid, words in doc2words.items():\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                wordid = dictionary[word]\n",
    "            else:\n",
    "                wordid = UNK\n",
    "                count[UNK][1] += 1\n",
    "            data.append((docid, wordid))\n",
    "        # Pad with NULL values if necessary\n",
    "        doclen = len(words)\n",
    "        doclens.append(doclen)\n",
    "        if doclen < TEXT_WINDOW_SIZE:\n",
    "            n_nulls = TEXT_WINDOW_SIZE - doclen\n",
    "            data.extend([(docid, NULL)] * n_nulls)\n",
    "            count[NULL][1] += n_nulls\n",
    "    return data, count, doclens, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, count, doclens, dictionary, reverse_dictionary = \\\n",
    "        build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 100\n",
      "Number of tokens: 15955\n",
      "Number of unique tokens: 3321\n",
      "Most common words (+UNK and NULL): [['__UNK__', 0], ['__NULL__', 0], ('the', 832), ('of', 466), ('to', 411)]\n",
      "Least common words: [('computer-related', 1), ('context', 1), ('undertook', 1), ('seoul', 1), ('seriousness', 1)]\n",
      "Sample data: [(0, 1471), (0, 846), (0, 2748), (0, 479), (0, 25)]\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents:', len(set(next(zip(*data)))))\n",
    "print('Number of tokens:', len(data))\n",
    "print('Number of unique tokens:', len(count))\n",
    "assert len(data) == sum([i for _, i in count])\n",
    "print('Most common words (+UNK and NULL):', count[:5])\n",
    "print('Least common words:', count[-5:])\n",
    "print('Sample data:', data[:5])\n",
    "\n",
    "vocab_size = min(VOCAB_SIZE, len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean     159.550000\n",
       "std      167.645271\n",
       "min       20.000000\n",
       "25%       52.500000\n",
       "50%      106.500000\n",
       "75%      198.250000\n",
       "max      779.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(doclens).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_window_center_positions():\n",
    "    # If TEXT_WINDOW_SIZE is even, then define text_window_center\n",
    "    # as left-of-middle-pair\n",
    "    doc_start_indexes = [0]\n",
    "    last_docid = data[0][0]\n",
    "    for i, (d, _) in enumerate(data):\n",
    "        if d != last_docid:\n",
    "            doc_start_indexes.append(i)\n",
    "            last_docid = d\n",
    "    twcp = []\n",
    "    for i in range(len(doc_start_indexes) - 1):\n",
    "        twcp.extend(list(range(\n",
    "                doc_start_indexes[i] + (TEXT_WINDOW_SIZE - 1) // 2,\n",
    "                doc_start_indexes[i + 1] - TEXT_WINDOW_SIZE // 2\n",
    "                )))\n",
    "    return doc_start_indexes, twcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_start_indexes, twcp = get_text_window_center_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test():\n",
    "    global twcp\n",
    "    np.random.shuffle(twcp)\n",
    "    split_point = (len(twcp) // 100) * (100 - PV_TEST_SET_PERCENTAGE)\n",
    "    twcp_train = twcp[:split_point]\n",
    "\n",
    "    # Test set data must come from known documents\n",
    "    docids_train = set([data[i][0] for i in twcp_train])\n",
    "    twcp_test = []\n",
    "    twcp_test_reject = []\n",
    "    for i in twcp[split_point:]:\n",
    "        if data[i][0] in docids_train:\n",
    "            twcp_test.append(i)\n",
    "        else:\n",
    "            twcp_test_reject.append(i)\n",
    "    twcp_train.extend(twcp_test_reject)\n",
    "    if not twcp_test:\n",
    "        raise ValueError(\n",
    "            'No test data, try increasing PV_TEST_SET_PERCENTAGE')\n",
    "    return twcp_train, twcp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twcp_train, twcp_test = get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective test set percentage: 811 out of 15061, 5.4%\n"
     ]
    }
   ],
   "source": [
    "print('Effective test set percentage: {} out of {}, {:.1f}%'.format(\n",
    "        len(twcp_test), len(twcp_test) + len(twcp_train),\n",
    "        100 * len(twcp_test) / (len(twcp_test) + len(twcp_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(twcp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twcp_train_index = 0\n",
    "\n",
    "def generate_batch_single_twcp(twcp, i, batch, labels):\n",
    "    tw_start = twcp - (TEXT_WINDOW_SIZE - 1) // 2\n",
    "    tw_end = twcp + TEXT_WINDOW_SIZE // 2 + 1\n",
    "    docids, wordids = zip(*data[tw_start:tw_end])\n",
    "    batch_slice = slice(i * TEXT_WINDOW_SIZE,\n",
    "                        (i+1) * TEXT_WINDOW_SIZE)\n",
    "    batch[batch_slice] = docids\n",
    "    labels[batch_slice, 0] = wordids\n",
    "    \n",
    "def generate_batch():\n",
    "    global twcp_train_index\n",
    "    batch = np.ndarray(shape=(BATCH_SIZE,), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(BATCH_SIZE, 1), dtype=np.int32)\n",
    "    for i in range(BATCH_SIZE // TEXT_WINDOW_SIZE):\n",
    "        generate_batch_single_twcp(twcp_train[twcp_train_index],\n",
    "                                   i, batch, labels)\n",
    "        twcp_train_index = (twcp_train_index + TEXT_WINDOW_SIZE) \\\n",
    "                % len(twcp_train)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch, labels = generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dataset_ = np.ndarray(shape=(len(twcp_test) * TEXT_WINDOW_SIZE,),\n",
    "                          dtype=np.int32)\n",
    "test_labels_ = np.ndarray(shape=(len(twcp_test) * TEXT_WINDOW_SIZE,\n",
    "                                1),\n",
    "                         dtype=np.int32)\n",
    "for i in range(len(twcp_test)):\n",
    "    generate_batch_single_twcp(twcp_test[i], i, test_dataset_, \n",
    "                              test_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "    # Input data\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "    test_dataset = tf.constant(test_dataset_, dtype=tf.int32)\n",
    "    test_labels = tf.constant(test_labels_, dtype=tf.int32)\n",
    "    \n",
    "    # Weights\n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([len(doclens), EMBEDDING_SIZE],\n",
    "                              -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                    [vocab_size, EMBEDDING_SIZE],\n",
    "                    stddev=1.0 / np.sqrt(EMBEDDING_SIZE)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "    \n",
    "    # Model\n",
    "    # Look up embeddings for inputs\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative\n",
    "    # labels each time\n",
    "    loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(\n",
    "                    softmax_weights, softmax_biases, embed,\n",
    "                    train_labels, NUM_SAMPLED, vocab_size))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(\n",
    "            loss)\n",
    "    \n",
    "    # Test loss\n",
    "    test_embed = tf.nn.embedding_lookup(embeddings, test_dataset)\n",
    "    test_loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    tf.matmul(test_embed, tf.transpose(\n",
    "                              softmax_weights)) + softmax_biases,\n",
    "                    test_labels[:, 0]))\n",
    "    \n",
    "    # Normalized embeddings (to use cosine similarity later on)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1,\n",
    "                                  keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 5.7\n",
      "Test loss at step 0: 8.2\n",
      "Average loss at step 100: 5.4\n",
      "Test loss at step 100: 7.7\n",
      "Average loss at step 200: 4.9\n",
      "Test loss at step 200: 7.3\n",
      "Average loss at step 300: 4.6\n",
      "Test loss at step 300: 7.0\n",
      "Average loss at step 400: 4.3\n",
      "Test loss at step 400: 6.8\n",
      "Average loss at step 500: 4.2\n",
      "Test loss at step 500: 6.7\n",
      "Average loss at step 600: 4.0\n",
      "Test loss at step 600: 6.5\n",
      "Average loss at step 700: 3.9\n",
      "Test loss at step 700: 6.4\n",
      "Average loss at step 800: 3.7\n",
      "Test loss at step 800: 6.3\n",
      "Average loss at step 900: 3.6\n",
      "Test loss at step 900: 6.2\n",
      "Average loss at step 1000: 3.5\n",
      "Test loss at step 1000: 6.2\n",
      "Average loss at step 1100: 3.5\n",
      "Test loss at step 1100: 6.1\n",
      "Average loss at step 1200: 3.4\n",
      "Test loss at step 1200: 6.0\n",
      "Average loss at step 1300: 3.4\n",
      "Test loss at step 1300: 6.0\n",
      "Average loss at step 1400: 3.3\n",
      "Test loss at step 1400: 5.9\n",
      "Average loss at step 1500: 3.2\n",
      "Test loss at step 1500: 5.9\n",
      "Average loss at step 1600: 3.2\n",
      "Test loss at step 1600: 5.8\n",
      "Average loss at step 1700: 3.1\n",
      "Test loss at step 1700: 5.8\n",
      "Average loss at step 1800: 3.1\n",
      "Test loss at step 1800: 5.8\n",
      "Average loss at step 1900: 3.1\n",
      "Test loss at step 1900: 5.7\n",
      "Average loss at step 2000: 3.0\n",
      "Test loss at step 2000: 5.7\n",
      "Average loss at step 2100: 3.0\n",
      "Test loss at step 2100: 5.7\n",
      "Average loss at step 2200: 2.9\n",
      "Test loss at step 2200: 5.6\n",
      "Average loss at step 2300: 2.9\n",
      "Test loss at step 2300: 5.6\n",
      "Average loss at step 2400: 2.9\n",
      "Test loss at step 2400: 5.6\n",
      "Average loss at step 2500: 2.8\n",
      "Test loss at step 2500: 5.6\n",
      "Average loss at step 2600: 2.9\n",
      "Test loss at step 2600: 5.5\n",
      "Average loss at step 2700: 2.8\n",
      "Test loss at step 2700: 5.5\n",
      "Average loss at step 2800: 2.8\n",
      "Test loss at step 2800: 5.5\n",
      "Average loss at step 2900: 2.8\n",
      "Test loss at step 2900: 5.5\n",
      "Average loss at step 3000: 2.7\n",
      "Test loss at step 3000: 5.4\n",
      "Average loss at step 3100: 2.7\n",
      "Test loss at step 3100: 5.4\n",
      "Average loss at step 3200: 2.7\n",
      "Test loss at step 3200: 5.4\n",
      "Average loss at step 3300: 2.7\n",
      "Test loss at step 3300: 5.4\n",
      "Average loss at step 3400: 2.7\n",
      "Test loss at step 3400: 5.4\n",
      "Average loss at step 3500: 2.7\n",
      "Test loss at step 3500: 5.4\n",
      "Average loss at step 3600: 2.6\n",
      "Test loss at step 3600: 5.4\n",
      "Average loss at step 3700: 2.6\n",
      "Test loss at step 3700: 5.3\n",
      "Average loss at step 3800: 2.6\n",
      "Test loss at step 3800: 5.3\n",
      "Average loss at step 3900: 2.6\n",
      "Test loss at step 3900: 5.3\n",
      "Average loss at step 4000: 2.6\n",
      "Test loss at step 4000: 5.3\n",
      "Average loss at step 4100: 2.6\n",
      "Test loss at step 4100: 5.3\n",
      "Average loss at step 4200: 2.6\n",
      "Test loss at step 4200: 5.3\n",
      "Average loss at step 4300: 2.5\n",
      "Test loss at step 4300: 5.3\n",
      "Average loss at step 4400: 2.6\n",
      "Test loss at step 4400: 5.3\n",
      "Average loss at step 4500: 2.6\n",
      "Test loss at step 4500: 5.2\n",
      "Average loss at step 4600: 2.5\n",
      "Test loss at step 4600: 5.2\n",
      "Average loss at step 4700: 2.5\n",
      "Test loss at step 4700: 5.2\n",
      "Average loss at step 4800: 2.5\n",
      "Test loss at step 4800: 5.2\n",
      "Average loss at step 4900: 2.5\n",
      "Test loss at step 4900: 5.2\n",
      "Average loss at step 5000: 2.5\n",
      "Test loss at step 5000: 5.2\n",
      "Average loss at step 5100: 2.5\n",
      "Test loss at step 5100: 5.2\n",
      "Average loss at step 5200: 2.5\n",
      "Test loss at step 5200: 5.2\n",
      "Average loss at step 5300: 2.5\n",
      "Test loss at step 5300: 5.2\n",
      "Average loss at step 5400: 2.5\n",
      "Test loss at step 5400: 5.2\n",
      "Average loss at step 5500: 2.5\n",
      "Test loss at step 5500: 5.2\n",
      "Average loss at step 5600: 2.5\n",
      "Test loss at step 5600: 5.2\n",
      "Average loss at step 5700: 2.4\n",
      "Test loss at step 5700: 5.2\n",
      "Average loss at step 5800: 2.5\n",
      "Test loss at step 5800: 5.2\n",
      "Average loss at step 5900: 2.5\n",
      "Test loss at step 5900: 5.1\n",
      "Average loss at step 6000: 2.4\n",
      "Test loss at step 6000: 5.1\n",
      "Average loss at step 6100: 2.4\n",
      "Test loss at step 6100: 5.1\n",
      "Average loss at step 6200: 2.4\n",
      "Test loss at step 6200: 5.1\n",
      "Average loss at step 6300: 2.4\n",
      "Test loss at step 6300: 5.1\n",
      "Average loss at step 6400: 2.4\n",
      "Test loss at step 6400: 5.1\n",
      "Average loss at step 6500: 2.4\n",
      "Test loss at step 6500: 5.1\n",
      "Average loss at step 6600: 2.5\n",
      "Test loss at step 6600: 5.1\n",
      "Average loss at step 6700: 2.4\n",
      "Test loss at step 6700: 5.1\n",
      "Average loss at step 6800: 2.4\n",
      "Test loss at step 6800: 5.1\n",
      "Average loss at step 6900: 2.4\n",
      "Test loss at step 6900: 5.1\n",
      "Average loss at step 7000: 2.4\n",
      "Test loss at step 7000: 5.1\n",
      "Average loss at step 7100: 2.4\n",
      "Test loss at step 7100: 5.1\n",
      "Average loss at step 7200: 2.4\n",
      "Test loss at step 7200: 5.1\n",
      "Average loss at step 7300: 2.4\n",
      "Test loss at step 7300: 5.1\n",
      "Average loss at step 7400: 2.4\n",
      "Test loss at step 7400: 5.1\n",
      "Average loss at step 7500: 2.4\n",
      "Test loss at step 7500: 5.1\n",
      "Average loss at step 7600: 2.4\n",
      "Test loss at step 7600: 5.1\n",
      "Average loss at step 7700: 2.4\n",
      "Test loss at step 7700: 5.1\n",
      "Average loss at step 7800: 2.4\n",
      "Test loss at step 7800: 5.1\n",
      "Average loss at step 7900: 2.4\n",
      "Test loss at step 7900: 5.1\n",
      "Average loss at step 8000: 2.4\n",
      "Test loss at step 8000: 5.1\n",
      "Average loss at step 8100: 2.4\n",
      "Test loss at step 8100: 5.1\n",
      "Average loss at step 8200: 2.3\n",
      "Test loss at step 8200: 5.1\n",
      "Average loss at step 8300: 2.4\n",
      "Test loss at step 8300: 5.1\n",
      "Average loss at step 8400: 2.3\n",
      "Test loss at step 8400: 5.1\n",
      "Average loss at step 8500: 2.4\n",
      "Test loss at step 8500: 5.1\n",
      "Average loss at step 8600: 2.4\n",
      "Test loss at step 8600: 5.0\n",
      "Average loss at step 8700: 2.4\n",
      "Test loss at step 8700: 5.0\n",
      "Average loss at step 8800: 2.4\n",
      "Test loss at step 8800: 5.0\n",
      "Average loss at step 8900: 2.4\n",
      "Test loss at step 8900: 5.0\n",
      "Average loss at step 9000: 2.4\n",
      "Test loss at step 9000: 5.0\n",
      "Average loss at step 9100: 2.3\n",
      "Test loss at step 9100: 5.0\n",
      "Average loss at step 9200: 2.3\n",
      "Test loss at step 9200: 5.0\n",
      "Average loss at step 9300: 2.3\n",
      "Test loss at step 9300: 5.0\n",
      "Average loss at step 9400: 2.4\n",
      "Test loss at step 9400: 5.0\n",
      "Average loss at step 9500: 2.4\n",
      "Test loss at step 9500: 5.0\n",
      "Average loss at step 9600: 2.3\n",
      "Test loss at step 9600: 5.0\n",
      "Average loss at step 9700: 2.4\n",
      "Test loss at step 9700: 5.0\n",
      "Average loss at step 9800: 2.3\n",
      "Test loss at step 9800: 5.0\n",
      "Average loss at step 9900: 2.4\n",
      "Test loss at step 9900: 5.0\n",
      "Average loss at step 10000: 2.3\n",
      "Test loss at step 10000: 5.0\n",
      "Average loss at step 10100: 2.4\n",
      "Test loss at step 10100: 5.0\n",
      "Average loss at step 10200: 2.4\n",
      "Test loss at step 10200: 5.0\n",
      "Average loss at step 10300: 2.3\n",
      "Test loss at step 10300: 5.0\n",
      "Average loss at step 10400: 2.4\n",
      "Test loss at step 10400: 5.0\n",
      "Average loss at step 10500: 2.3\n",
      "Test loss at step 10500: 5.0\n",
      "Average loss at step 10600: 2.4\n",
      "Test loss at step 10600: 5.0\n",
      "Average loss at step 10700: 2.3\n",
      "Test loss at step 10700: 5.0\n",
      "Average loss at step 10800: 2.4\n",
      "Test loss at step 10800: 5.0\n",
      "Average loss at step 10900: 2.4\n",
      "Test loss at step 10900: 5.0\n",
      "Average loss at step 11000: 2.3\n",
      "Test loss at step 11000: 5.0\n",
      "Average loss at step 11100: 2.3\n",
      "Test loss at step 11100: 5.0\n",
      "Average loss at step 11200: 2.3\n",
      "Test loss at step 11200: 5.0\n",
      "Average loss at step 11300: 2.3\n",
      "Test loss at step 11300: 5.0\n",
      "Average loss at step 11400: 2.3\n",
      "Test loss at step 11400: 5.0\n",
      "Average loss at step 11500: 2.4\n",
      "Test loss at step 11500: 5.0\n",
      "Average loss at step 11600: 2.4\n",
      "Test loss at step 11600: 5.0\n",
      "Average loss at step 11700: 2.3\n",
      "Test loss at step 11700: 5.0\n",
      "Average loss at step 11800: 2.3\n",
      "Test loss at step 11800: 5.0\n",
      "Average loss at step 11900: 2.3\n",
      "Test loss at step 11900: 5.0\n",
      "Average loss at step 12000: 2.3\n",
      "Test loss at step 12000: 5.0\n",
      "Average loss at step 12100: 2.3\n",
      "Test loss at step 12100: 5.0\n",
      "Average loss at step 12200: 2.4\n",
      "Test loss at step 12200: 5.0\n",
      "Average loss at step 12300: 2.4\n",
      "Test loss at step 12300: 5.0\n",
      "Average loss at step 12400: 2.3\n",
      "Test loss at step 12400: 5.0\n",
      "Average loss at step 12500: 2.3\n",
      "Test loss at step 12500: 5.0\n",
      "Average loss at step 12600: 2.3\n",
      "Test loss at step 12600: 5.0\n",
      "Average loss at step 12700: 2.3\n",
      "Test loss at step 12700: 5.0\n",
      "Average loss at step 12800: 2.3\n",
      "Test loss at step 12800: 5.0\n",
      "Average loss at step 12900: 2.3\n",
      "Test loss at step 12900: 5.0\n",
      "Average loss at step 13000: 2.4\n",
      "Test loss at step 13000: 5.0\n",
      "Average loss at step 13100: 2.3\n",
      "Test loss at step 13100: 5.0\n",
      "Average loss at step 13200: 2.3\n",
      "Test loss at step 13200: 5.0\n",
      "Average loss at step 13300: 2.3\n",
      "Test loss at step 13300: 5.0\n",
      "Average loss at step 13400: 2.3\n",
      "Test loss at step 13400: 5.0\n",
      "Average loss at step 13500: 2.3\n",
      "Test loss at step 13500: 5.0\n",
      "Average loss at step 13600: 2.3\n",
      "Test loss at step 13600: 5.0\n",
      "Average loss at step 13700: 2.3\n",
      "Test loss at step 13700: 5.0\n",
      "Average loss at step 13800: 2.4\n",
      "Test loss at step 13800: 5.0\n",
      "Average loss at step 13900: 2.3\n",
      "Test loss at step 13900: 5.0\n",
      "Average loss at step 14000: 2.3\n",
      "Test loss at step 14000: 5.0\n",
      "Average loss at step 14100: 2.3\n",
      "Test loss at step 14100: 5.0\n",
      "Average loss at step 14200: 2.3\n",
      "Test loss at step 14200: 5.0\n",
      "Average loss at step 14300: 2.3\n",
      "Test loss at step 14300: 5.0\n",
      "Average loss at step 14400: 2.3\n",
      "Test loss at step 14400: 5.0\n",
      "Average loss at step 14500: 2.3\n",
      "Test loss at step 14500: 4.9\n",
      "Average loss at step 14600: 2.3\n",
      "Test loss at step 14600: 4.9\n",
      "Average loss at step 14700: 2.3\n",
      "Test loss at step 14700: 4.9\n",
      "Average loss at step 14800: 2.3\n",
      "Test loss at step 14800: 4.9\n",
      "Average loss at step 14900: 2.3\n",
      "Test loss at step 14900: 4.9\n",
      "Average loss at step 15000: 2.3\n",
      "Test loss at step 15000: 4.9\n",
      "Average loss at step 15100: 2.3\n",
      "Test loss at step 15100: 4.9\n",
      "Average loss at step 15200: 2.3\n",
      "Test loss at step 15200: 4.9\n",
      "Average loss at step 15300: 2.3\n",
      "Test loss at step 15300: 4.9\n",
      "Average loss at step 15400: 2.3\n",
      "Test loss at step 15400: 4.9\n",
      "Average loss at step 15500: 2.3\n",
      "Test loss at step 15500: 4.9\n",
      "Average loss at step 15600: 2.3\n",
      "Test loss at step 15600: 4.9\n",
      "Average loss at step 15700: 2.3\n",
      "Test loss at step 15700: 5.0\n",
      "Average loss at step 15800: 2.3\n",
      "Test loss at step 15800: 4.9\n",
      "Average loss at step 15900: 2.3\n",
      "Test loss at step 15900: 4.9\n",
      "Average loss at step 16000: 2.3\n",
      "Test loss at step 16000: 4.9\n",
      "Average loss at step 16100: 2.3\n",
      "Test loss at step 16100: 4.9\n",
      "Average loss at step 16200: 2.3\n",
      "Test loss at step 16200: 4.9\n",
      "Average loss at step 16300: 2.3\n",
      "Test loss at step 16300: 4.9\n",
      "Average loss at step 16400: 2.3\n",
      "Test loss at step 16400: 4.9\n",
      "Average loss at step 16500: 2.3\n",
      "Test loss at step 16500: 5.0\n",
      "Average loss at step 16600: 2.3\n",
      "Test loss at step 16600: 4.9\n",
      "Average loss at step 16700: 2.3\n",
      "Test loss at step 16700: 4.9\n",
      "Average loss at step 16800: 2.3\n",
      "Test loss at step 16800: 4.9\n",
      "Average loss at step 16900: 2.3\n",
      "Test loss at step 16900: 4.9\n",
      "Average loss at step 17000: 2.3\n",
      "Test loss at step 17000: 4.9\n",
      "Average loss at step 17100: 2.3\n",
      "Test loss at step 17100: 4.9\n",
      "Average loss at step 17200: 2.3\n",
      "Test loss at step 17200: 4.9\n",
      "Average loss at step 17300: 2.4\n",
      "Test loss at step 17300: 4.9\n",
      "Average loss at step 17400: 2.3\n",
      "Test loss at step 17400: 4.9\n",
      "Average loss at step 17500: 2.3\n",
      "Test loss at step 17500: 4.9\n",
      "Average loss at step 17600: 2.3\n",
      "Test loss at step 17600: 4.9\n",
      "Average loss at step 17700: 2.3\n",
      "Test loss at step 17700: 4.9\n",
      "Average loss at step 17800: 2.3\n",
      "Test loss at step 17800: 4.9\n",
      "Average loss at step 17900: 2.3\n",
      "Test loss at step 17900: 4.9\n",
      "Average loss at step 18000: 2.4\n",
      "Test loss at step 18000: 4.9\n",
      "Average loss at step 18100: 2.3\n",
      "Test loss at step 18100: 4.9\n",
      "Average loss at step 18200: 2.3\n",
      "Test loss at step 18200: 4.9\n",
      "Average loss at step 18300: 2.3\n",
      "Test loss at step 18300: 4.9\n",
      "Average loss at step 18400: 2.3\n",
      "Test loss at step 18400: 4.9\n",
      "Average loss at step 18500: 2.3\n",
      "Test loss at step 18500: 4.9\n",
      "Average loss at step 18600: 2.3\n",
      "Test loss at step 18600: 4.9\n",
      "Average loss at step 18700: 2.4\n",
      "Test loss at step 18700: 4.9\n",
      "Average loss at step 18800: 2.3\n",
      "Test loss at step 18800: 4.9\n",
      "Average loss at step 18900: 2.3\n",
      "Test loss at step 18900: 4.9\n",
      "Average loss at step 19000: 2.3\n",
      "Test loss at step 19000: 4.9\n",
      "Average loss at step 19100: 2.3\n",
      "Test loss at step 19100: 4.9\n",
      "Average loss at step 19200: 2.3\n",
      "Test loss at step 19200: 4.9\n",
      "Average loss at step 19300: 2.3\n",
      "Test loss at step 19300: 4.9\n",
      "Average loss at step 19400: 2.3\n",
      "Test loss at step 19400: 4.9\n",
      "Average loss at step 19500: 2.4\n",
      "Test loss at step 19500: 4.9\n",
      "Average loss at step 19600: 2.3\n",
      "Test loss at step 19600: 4.9\n",
      "Average loss at step 19700: 2.3\n",
      "Test loss at step 19700: 4.9\n",
      "Average loss at step 19800: 2.2\n",
      "Test loss at step 19800: 4.9\n",
      "Average loss at step 19900: 2.3\n",
      "Test loss at step 19900: 4.9\n",
      "Average loss at step 20000: 2.3\n",
      "Test loss at step 20000: 4.9\n",
      "Average loss at step 20100: 2.3\n",
      "Test loss at step 20100: 4.9\n",
      "Average loss at step 20200: 2.3\n",
      "Test loss at step 20200: 4.9\n",
      "Average loss at step 20300: 2.3\n",
      "Test loss at step 20300: 4.9\n",
      "Average loss at step 20400: 2.3\n",
      "Test loss at step 20400: 4.9\n",
      "Average loss at step 20500: 2.3\n",
      "Test loss at step 20500: 4.9\n",
      "Average loss at step 20600: 2.3\n",
      "Test loss at step 20600: 4.9\n",
      "Average loss at step 20700: 2.3\n",
      "Test loss at step 20700: 4.9\n",
      "Average loss at step 20800: 2.3\n",
      "Test loss at step 20800: 4.9\n",
      "Average loss at step 20900: 2.3\n",
      "Test loss at step 20900: 4.9\n",
      "Average loss at step 21000: 2.3\n",
      "Test loss at step 21000: 4.9\n",
      "Average loss at step 21100: 2.3\n",
      "Test loss at step 21100: 4.9\n",
      "Average loss at step 21200: 2.3\n",
      "Test loss at step 21200: 4.9\n",
      "Average loss at step 21300: 2.3\n",
      "Test loss at step 21300: 4.9\n",
      "Average loss at step 21400: 2.3\n",
      "Test loss at step 21400: 4.9\n",
      "Average loss at step 21500: 2.3\n",
      "Test loss at step 21500: 4.9\n",
      "Average loss at step 21600: 2.3\n",
      "Test loss at step 21600: 4.9\n",
      "Average loss at step 21700: 2.3\n",
      "Test loss at step 21700: 4.9\n",
      "Average loss at step 21800: 2.3\n",
      "Test loss at step 21800: 4.9\n",
      "Average loss at step 21900: 2.3\n",
      "Test loss at step 21900: 4.9\n",
      "Average loss at step 22000: 2.3\n",
      "Test loss at step 22000: 4.9\n",
      "Average loss at step 22100: 2.3\n",
      "Test loss at step 22100: 4.9\n",
      "Average loss at step 22200: 2.3\n",
      "Test loss at step 22200: 4.9\n",
      "Average loss at step 22300: 2.3\n",
      "Test loss at step 22300: 4.9\n",
      "Average loss at step 22400: 2.3\n",
      "Test loss at step 22400: 4.9\n",
      "Average loss at step 22500: 2.3\n",
      "Test loss at step 22500: 4.9\n",
      "Average loss at step 22600: 2.3\n",
      "Test loss at step 22600: 4.9\n",
      "Average loss at step 22700: 2.3\n",
      "Test loss at step 22700: 4.9\n",
      "Average loss at step 22800: 2.3\n",
      "Test loss at step 22800: 4.9\n",
      "Average loss at step 22900: 2.3\n",
      "Test loss at step 22900: 4.9\n",
      "Average loss at step 23000: 2.3\n",
      "Test loss at step 23000: 4.9\n",
      "Average loss at step 23100: 2.3\n",
      "Test loss at step 23100: 4.9\n",
      "Average loss at step 23200: 2.3\n",
      "Test loss at step 23200: 4.9\n",
      "Average loss at step 23300: 2.3\n",
      "Test loss at step 23300: 4.9\n",
      "Average loss at step 23400: 2.2\n",
      "Test loss at step 23400: 4.9\n",
      "Average loss at step 23500: 2.3\n",
      "Test loss at step 23500: 4.9\n",
      "Average loss at step 23600: 2.3\n",
      "Test loss at step 23600: 4.9\n",
      "Average loss at step 23700: 2.3\n",
      "Test loss at step 23700: 4.9\n",
      "Average loss at step 23800: 2.3\n",
      "Test loss at step 23800: 4.9\n",
      "Average loss at step 23900: 2.3\n",
      "Test loss at step 23900: 4.9\n",
      "Average loss at step 24000: 2.3\n",
      "Test loss at step 24000: 4.9\n",
      "Average loss at step 24100: 2.3\n",
      "Test loss at step 24100: 4.9\n",
      "Average loss at step 24200: 2.3\n",
      "Test loss at step 24200: 4.9\n",
      "Average loss at step 24300: 2.3\n",
      "Test loss at step 24300: 4.9\n",
      "Average loss at step 24400: 2.3\n",
      "Test loss at step 24400: 4.9\n",
      "Average loss at step 24500: 2.3\n",
      "Test loss at step 24500: 4.9\n",
      "Average loss at step 24600: 2.3\n",
      "Test loss at step 24600: 4.9\n",
      "Average loss at step 24700: 2.3\n",
      "Test loss at step 24700: 4.9\n",
      "Average loss at step 24800: 2.3\n",
      "Test loss at step 24800: 4.9\n",
      "Average loss at step 24900: 2.3\n",
      "Test loss at step 24900: 4.9\n",
      "Average loss at step 25000: 2.3\n",
      "Test loss at step 25000: 4.9\n",
      "Average loss at step 25100: 2.3\n",
      "Test loss at step 25100: 4.9\n",
      "Average loss at step 25200: 2.3\n",
      "Test loss at step 25200: 4.9\n",
      "Average loss at step 25300: 2.2\n",
      "Test loss at step 25300: 4.9\n",
      "Average loss at step 25400: 2.3\n",
      "Test loss at step 25400: 4.9\n",
      "Average loss at step 25500: 2.2\n",
      "Test loss at step 25500: 4.9\n",
      "Average loss at step 25600: 2.3\n",
      "Test loss at step 25600: 4.9\n",
      "Average loss at step 25700: 2.3\n",
      "Test loss at step 25700: 4.9\n",
      "Average loss at step 25800: 2.3\n",
      "Test loss at step 25800: 4.9\n",
      "Average loss at step 25900: 2.3\n",
      "Test loss at step 25900: 4.9\n",
      "Average loss at step 26000: 2.2\n",
      "Test loss at step 26000: 4.9\n",
      "Average loss at step 26100: 2.3\n",
      "Test loss at step 26100: 4.9\n",
      "Average loss at step 26200: 2.2\n",
      "Test loss at step 26200: 4.9\n",
      "Average loss at step 26300: 2.3\n",
      "Test loss at step 26300: 4.9\n",
      "Average loss at step 26400: 2.2\n",
      "Test loss at step 26400: 4.9\n",
      "Average loss at step 26500: 2.3\n",
      "Test loss at step 26500: 4.9\n",
      "Average loss at step 26600: 2.3\n",
      "Test loss at step 26600: 4.9\n",
      "Average loss at step 26700: 2.2\n",
      "Test loss at step 26700: 4.9\n",
      "Average loss at step 26800: 2.3\n",
      "Test loss at step 26800: 4.9\n",
      "Average loss at step 26900: 2.3\n",
      "Test loss at step 26900: 4.9\n",
      "Average loss at step 27000: 2.3\n",
      "Test loss at step 27000: 4.9\n",
      "Average loss at step 27100: 2.3\n",
      "Test loss at step 27100: 4.9\n",
      "Average loss at step 27200: 2.3\n",
      "Test loss at step 27200: 4.9\n",
      "Average loss at step 27300: 2.3\n",
      "Test loss at step 27300: 4.9\n",
      "Average loss at step 27400: 2.3\n",
      "Test loss at step 27400: 4.9\n",
      "Average loss at step 27500: 2.3\n",
      "Test loss at step 27500: 4.9\n",
      "Average loss at step 27600: 2.2\n",
      "Test loss at step 27600: 4.9\n",
      "Average loss at step 27700: 2.3\n",
      "Test loss at step 27700: 4.9\n",
      "Average loss at step 27800: 2.2\n",
      "Test loss at step 27800: 4.9\n",
      "Average loss at step 27900: 2.3\n",
      "Test loss at step 27900: 4.9\n",
      "Average loss at step 28000: 2.3\n",
      "Test loss at step 28000: 4.9\n",
      "Average loss at step 28100: 2.3\n",
      "Test loss at step 28100: 4.9\n",
      "Average loss at step 28200: 2.3\n",
      "Test loss at step 28200: 4.9\n",
      "Average loss at step 28300: 2.2\n",
      "Test loss at step 28300: 4.9\n",
      "Average loss at step 28400: 2.3\n",
      "Test loss at step 28400: 4.9\n",
      "Average loss at step 28500: 2.2\n",
      "Test loss at step 28500: 4.9\n",
      "Average loss at step 28600: 2.3\n",
      "Test loss at step 28600: 4.9\n",
      "Average loss at step 28700: 2.3\n",
      "Test loss at step 28700: 4.9\n",
      "Average loss at step 28800: 2.2\n",
      "Test loss at step 28800: 4.9\n",
      "Average loss at step 28900: 2.3\n",
      "Test loss at step 28900: 4.9\n",
      "Average loss at step 29000: 2.2\n",
      "Test loss at step 29000: 4.9\n",
      "Average loss at step 29100: 2.3\n",
      "Test loss at step 29100: 4.9\n",
      "Average loss at step 29200: 2.2\n",
      "Test loss at step 29200: 4.9\n",
      "Average loss at step 29300: 2.3\n",
      "Test loss at step 29300: 4.9\n",
      "Average loss at step 29400: 2.3\n",
      "Test loss at step 29400: 4.9\n",
      "Average loss at step 29500: 2.3\n",
      "Test loss at step 29500: 4.9\n",
      "Average loss at step 29600: 2.3\n",
      "Test loss at step 29600: 4.9\n",
      "Average loss at step 29700: 2.3\n",
      "Test loss at step 29700: 4.9\n",
      "Average loss at step 29800: 2.3\n",
      "Test loss at step 29800: 4.9\n",
      "Average loss at step 29900: 2.2\n",
      "Test loss at step 29900: 4.9\n",
      "Average loss at step 30000: 2.3\n",
      "Test loss at step 30000: 4.9\n",
      "Average loss at step 30100: 2.3\n",
      "Test loss at step 30100: 4.9\n",
      "Average loss at step 30200: 2.3\n",
      "Test loss at step 30200: 4.9\n",
      "Average loss at step 30300: 2.2\n",
      "Test loss at step 30300: 4.9\n",
      "Average loss at step 30400: 2.3\n",
      "Test loss at step 30400: 4.9\n",
      "Average loss at step 30500: 2.3\n",
      "Test loss at step 30500: 4.9\n",
      "Average loss at step 30600: 2.3\n",
      "Test loss at step 30600: 4.9\n",
      "Average loss at step 30700: 2.3\n",
      "Test loss at step 30700: 4.9\n",
      "Average loss at step 30800: 2.3\n",
      "Test loss at step 30800: 4.9\n",
      "Average loss at step 30900: 2.3\n",
      "Test loss at step 30900: 4.9\n",
      "Average loss at step 31000: 2.3\n",
      "Test loss at step 31000: 4.9\n",
      "Average loss at step 31100: 2.3\n",
      "Test loss at step 31100: 4.9\n",
      "Average loss at step 31200: 2.2\n",
      "Test loss at step 31200: 4.9\n",
      "Average loss at step 31300: 2.2\n",
      "Test loss at step 31300: 4.9\n",
      "Average loss at step 31400: 2.3\n",
      "Test loss at step 31400: 4.9\n",
      "Average loss at step 31500: 2.3\n",
      "Test loss at step 31500: 4.9\n",
      "Average loss at step 31600: 2.3\n",
      "Test loss at step 31600: 4.9\n",
      "Average loss at step 31700: 2.2\n",
      "Test loss at step 31700: 4.9\n",
      "Average loss at step 31800: 2.3\n",
      "Test loss at step 31800: 4.9\n",
      "Average loss at step 31900: 2.2\n",
      "Test loss at step 31900: 4.9\n",
      "Average loss at step 32000: 2.3\n",
      "Test loss at step 32000: 4.9\n",
      "Average loss at step 32100: 2.2\n",
      "Test loss at step 32100: 4.9\n",
      "Average loss at step 32200: 2.3\n",
      "Test loss at step 32200: 4.9\n",
      "Average loss at step 32300: 2.3\n",
      "Test loss at step 32300: 4.9\n",
      "Average loss at step 32400: 2.2\n",
      "Test loss at step 32400: 4.9\n",
      "Average loss at step 32500: 2.3\n",
      "Test loss at step 32500: 4.9\n",
      "Average loss at step 32600: 2.2\n",
      "Test loss at step 32600: 4.9\n",
      "Average loss at step 32700: 2.3\n",
      "Test loss at step 32700: 4.9\n",
      "Average loss at step 32800: 2.3\n",
      "Test loss at step 32800: 4.9\n",
      "Average loss at step 32900: 2.3\n",
      "Test loss at step 32900: 4.9\n",
      "Average loss at step 33000: 2.3\n",
      "Test loss at step 33000: 4.9\n",
      "Average loss at step 33100: 2.2\n",
      "Test loss at step 33100: 4.9\n",
      "Average loss at step 33200: 2.3\n",
      "Test loss at step 33200: 4.9\n",
      "Average loss at step 33300: 2.2\n",
      "Test loss at step 33300: 4.9\n",
      "Average loss at step 33400: 2.3\n",
      "Test loss at step 33400: 4.9\n",
      "Average loss at step 33500: 2.2\n",
      "Test loss at step 33500: 4.9\n",
      "Average loss at step 33600: 2.3\n",
      "Test loss at step 33600: 4.9\n",
      "Average loss at step 33700: 2.3\n",
      "Test loss at step 33700: 4.9\n",
      "Average loss at step 33800: 2.2\n",
      "Test loss at step 33800: 4.9\n",
      "Average loss at step 33900: 2.3\n",
      "Test loss at step 33900: 4.9\n",
      "Average loss at step 34000: 2.2\n",
      "Test loss at step 34000: 4.9\n",
      "Average loss at step 34100: 2.3\n",
      "Test loss at step 34100: 4.9\n",
      "Average loss at step 34200: 2.2\n",
      "Test loss at step 34200: 4.9\n",
      "Average loss at step 34300: 2.3\n",
      "Test loss at step 34300: 4.9\n",
      "Average loss at step 34400: 2.3\n",
      "Test loss at step 34400: 4.9\n",
      "Average loss at step 34500: 2.2\n",
      "Test loss at step 34500: 4.9\n",
      "Average loss at step 34600: 2.2\n",
      "Test loss at step 34600: 4.9\n",
      "Average loss at step 34700: 2.2\n",
      "Test loss at step 34700: 4.9\n",
      "Average loss at step 34800: 2.2\n",
      "Test loss at step 34800: 4.9\n",
      "Average loss at step 34900: 2.2\n",
      "Test loss at step 34900: 4.9\n",
      "Average loss at step 35000: 2.3\n",
      "Test loss at step 35000: 4.9\n",
      "Average loss at step 35100: 2.3\n",
      "Test loss at step 35100: 4.9\n",
      "Average loss at step 35200: 2.3\n",
      "Test loss at step 35200: 4.9\n",
      "Average loss at step 35300: 2.3\n",
      "Test loss at step 35300: 4.9\n",
      "Average loss at step 35400: 2.3\n",
      "Test loss at step 35400: 4.9\n",
      "Average loss at step 35500: 2.2\n",
      "Test loss at step 35500: 4.9\n",
      "Average loss at step 35600: 2.2\n",
      "Test loss at step 35600: 4.9\n",
      "Average loss at step 35700: 2.3\n",
      "Test loss at step 35700: 4.9\n",
      "Average loss at step 35800: 2.3\n",
      "Test loss at step 35800: 4.9\n",
      "Average loss at step 35900: 2.3\n",
      "Test loss at step 35900: 4.9\n",
      "Average loss at step 36000: 2.3\n",
      "Test loss at step 36000: 4.9\n",
      "Average loss at step 36100: 2.2\n",
      "Test loss at step 36100: 4.9\n",
      "Average loss at step 36200: 2.2\n",
      "Test loss at step 36200: 4.9\n",
      "Average loss at step 36300: 2.2\n",
      "Test loss at step 36300: 4.9\n",
      "Average loss at step 36400: 2.3\n",
      "Test loss at step 36400: 4.9\n",
      "Average loss at step 36500: 2.3\n",
      "Test loss at step 36500: 4.9\n",
      "Average loss at step 36600: 2.3\n",
      "Test loss at step 36600: 4.9\n",
      "Average loss at step 36700: 2.2\n",
      "Test loss at step 36700: 4.9\n",
      "Average loss at step 36800: 2.3\n",
      "Test loss at step 36800: 4.9\n",
      "Average loss at step 36900: 2.3\n",
      "Test loss at step 36900: 4.9\n",
      "Average loss at step 37000: 2.2\n",
      "Test loss at step 37000: 4.9\n",
      "Average loss at step 37100: 2.3\n",
      "Test loss at step 37100: 4.9\n",
      "Average loss at step 37200: 2.3\n",
      "Test loss at step 37200: 4.9\n",
      "Average loss at step 37300: 2.3\n",
      "Test loss at step 37300: 4.9\n",
      "Average loss at step 37400: 2.2\n",
      "Test loss at step 37400: 4.9\n",
      "Average loss at step 37500: 2.3\n",
      "Test loss at step 37500: 4.9\n",
      "Average loss at step 37600: 2.2\n",
      "Test loss at step 37600: 4.9\n",
      "Average loss at step 37700: 2.3\n",
      "Test loss at step 37700: 4.9\n",
      "Average loss at step 37800: 2.3\n",
      "Test loss at step 37800: 4.9\n",
      "Average loss at step 37900: 2.3\n",
      "Test loss at step 37900: 4.9\n",
      "Average loss at step 38000: 2.3\n",
      "Test loss at step 38000: 4.9\n",
      "Average loss at step 38100: 2.2\n",
      "Test loss at step 38100: 4.9\n",
      "Average loss at step 38200: 2.3\n",
      "Test loss at step 38200: 4.9\n",
      "Average loss at step 38300: 2.3\n",
      "Test loss at step 38300: 4.9\n",
      "Average loss at step 38400: 2.3\n",
      "Test loss at step 38400: 4.9\n",
      "Average loss at step 38500: 2.2\n",
      "Test loss at step 38500: 4.9"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    avg_training_loss = 0\n",
    "    for step in range(NUM_STEPS):\n",
    "        batch_data, batch_labels = generate_batch()\n",
    "        feed_dict = {train_dataset: batch_data,\n",
    "                     train_labels: batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        avg_training_loss += l\n",
    "        if step % REPORT_EVERY_X_STEPS == 0:\n",
    "            if step > 0:\n",
    "                avg_training_loss = \\\n",
    "                        avg_training_loss / REPORT_EVERY_X_STEPS\n",
    "            # The average loss is an estimate of the loss over the\n",
    "            # last REPORT_EVERY_X_STEPS batches\n",
    "            print('Average loss at step {:d}: {:.1f}'.format(\n",
    "                    step, avg_training_loss))\n",
    "            avg_training_loss = 0\n",
    "            test_l = test_loss.eval()\n",
    "            print('Test loss at step {:d}: {:.1f}'.format(\n",
    "                    step, test_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
